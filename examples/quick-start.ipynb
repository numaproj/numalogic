{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to generate some synthetic multivariate time series data, inject some anomalies into test data. And then use some Machine learning techniques to detect these anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 2]\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"agg.path.chunksize\"] = 100000\n",
    "pd.set_option(\"plotting.backend\", \"matplotlib\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numalogic provides a synthetic time series data generator, that can be used to experiment and evaluate different ML alogorithms. Here we are generaing 3 timeseries (num_series) with 8000 data points (seq_len).\n",
    "\n",
    "For more details on synthetic data generation, please refer to [data generator](../docs/data-generator.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numalogic.synthetic import SyntheticTSGenerator\n",
    "\n",
    "ts_generator = SyntheticTSGenerator(\n",
    "    seq_len=8000,\n",
    "    num_series=3,\n",
    "    freq=\"T\",\n",
    "    primary_period=720,\n",
    "    secondary_period=6000,\n",
    "    seasonal_ts_prob=1.0,\n",
    "    baseline_range=(200.0, 350.0),\n",
    "    slope_range=(-0.001, 0.01),\n",
    "    amplitude_range=(10, 75),\n",
    "    cosine_ratio_range=(0.5, 0.9),\n",
    "    noise_range=(5, 15),\n",
    ")\n",
    "\n",
    "# shape: (8000, 1) with column names [s1]\n",
    "ts_df = ts_generator.gen_tseries()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.plot(title=\"Time series data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the generated data into train and test dataframes, with test set size 1000 leaving 7000 datapoints for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = ts_generator.train_test_split(ts_df, test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_df[\"s1\"].plot(label=\"train\")\n",
    "test_df[\"s1\"].plot(ax=ax, label=\"test\", title=\"Time series before injecting anomalies\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Inject anomalies:\n",
    "\n",
    "Now, we inject anomalies into the test set using the `AnomalyGenerator`. Here we are adding `contextual` anomalies, you could also try `causal` or `collective` anomalies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numalogic.synthetic import AnomalyGenerator\n",
    "\n",
    "injected_cols = [\"s1\", \"s2\"]  # columns to inject anomalies\n",
    "anomaly_generator = AnomalyGenerator(train_df, anomaly_type=\"contextual\", anomaly_ratio=0.3)\n",
    "outliers_test_df = anomaly_generator.inject_anomalies(\n",
    "    test_df, cols=injected_cols, impact=1.5\n",
    ")\n",
    "outliers_test_df.drop(columns = \"is_anomaly\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = train_df[\"s1\"].plot(label=\"train\")\n",
    "outliers_test_df[\"s1\"].plot(ax=ax, label=\"test\", title=\"Time series after injecting anomalies\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing:\n",
    "\n",
    "Pre-processing is an optional step, where we can clean, scale or transform the data. We can transform the features by scaling each feature to a given range, while keeping the shape of the distribution same. \n",
    "\n",
    "Here we are using `MinMaxScaler`, you could also try other scalers like `StandarScaler` or `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_df.to_numpy())\n",
    "X_test = scaler.transform(outliers_test_df.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training step, we define the model and train it on the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from numalogic.models.autoencoder import AutoencoderTrainer\n",
    "from numalogic.models.autoencoder.variants import SparseConv1dAE, Conv1dAE, VanillaAE\n",
    "from numalogic.tools.data import StreamingDataset\n",
    "\n",
    "seq_len = 36\n",
    "\n",
    "model = SparseConv1dAE(seq_len=seq_len, in_channels=3, enc_channels=8)\n",
    "trainer = AutoencoderTrainer(max_epochs=30, enable_progress_bar=True)\n",
    "trainer.fit(model, train_dataloaders=DataLoader(StreamingDataset(X_train, seq_len=seq_len), batch_size=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Threshold calculation:\n",
    "\n",
    "After training the main model, we need to perform a threshold calculation. The autoencoder tries to encode the representation of the normal input data,\n",
    "and tries to reconstuct the output. The difference between the actual input and the reconstructed output is what we call as the reconstruction error.\n",
    "\n",
    "Some amount of recconstruction error is normal, and we need to know what amount is normal, and what can be called as an outlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate training set reconstruction error\n",
    "train_reconerr = trainer.predict(model, dataloaders=DataLoader(StreamingDataset(X_train, seq_len=seq_len)))\n",
    "train_reconerr.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we say that anything more than 3 times the standard deviation from the mean is anomalous."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the threshold\n",
    "from numalogic.models.threshold import StdDevThreshold\n",
    "\n",
    "threshold_clf = StdDevThreshold()\n",
    "threshold_clf.fit(train_reconerr.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference:\n",
    "\n",
    "Now, we use the trained model from above to predict anomalies in the test data. The predict method return the reconstruction error produced by the model. \n",
    "\n",
    "Score method returns the anomaly score, calculated using thresholds. If the score is less than 1, it indicates an inlier. And the score greater than 1 indicates an outlier or anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reconstruction error on the test set\n",
    "test_recon_err = trainer.predict(model, dataloaders=DataLoader(StreamingDataset(X_test, seq_len=seq_len)))\n",
    "test_recon_err.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the anomaly score using the threshold estimator.\n",
    "test_anomaly_score = threshold_clf.score_samples(test_recon_err.numpy())\n",
    "print(test_recon_err.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the score array into a dataframe\n",
    "anomalies_df = pd.DataFrame(data=test_anomaly_score, columns=outliers_test_df.columns, index=outliers_test_df.index)\n",
    "anomalies_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(outliers_test_df[\"s1\"])\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(anomalies_df[\"s1\"], color=\"orange\")\n",
    "plt.legend([l1, l2], [\"test data\", \"anomaly score\"])\n",
    "plt.title(\"S1 data vs anomaly score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post-processing:\n",
    "\n",
    "Post-processing step is an optional step, where we normalize the anomalies between 0-10. This can make the scores more human interpretable.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2942, 3])\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numalogic.postprocess import TanhNorm\n",
    "\n",
    "postproc_clf = TanhNorm()\n",
    "test_anomaly_score_norm = postproc_clf.fit_transform(test_anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_anomalies_df = pd.DataFrame(data=test_anomaly_score_norm, columns=outliers_test_df.columns, index=outliers_test_df.index)\n",
    "norm_anomalies_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(outliers_test_df[\"s1\"])\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(norm_anomalies_df[\"s1\"], color=\"orange\")\n",
    "plt.legend([l1, l2], [\"test data\", \"anomaly score\"])\n",
    "plt.title(\"S1 data vs normalized anomaly score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "# LSTM sentiment classifier\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
