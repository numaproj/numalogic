{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quick Start (single-variate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to generate some synthetic single variate time series data, inject some anomalies into test data. And then use some Machine learning techniques to detect these anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 2]\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"agg.path.chunksize\"] = 100000\n",
    "pd.set_option(\"plotting.backend\", \"matplotlib\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numalogic provides a synthetic time series data generator, that can be used to experiment and evaluate different ML alogorithms. Here we are generaing a single timeseries (num_series) with 9000 data points.\n",
    "\n",
    "For more details on synthetic data generation, please refer to [data generator](../docs/data-generator.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TOTAL_SIZE = 9000\n",
    "VALIDATION_SIZE = 1000\n",
    "TEST_SIZE = 1000"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "source": [
    "from numalogic.synthetic import SyntheticTSGenerator\n",
    "\n",
    "ts_generator = SyntheticTSGenerator(\n",
    "    seq_len=TOTAL_SIZE,\n",
    "    num_series=1,\n",
    "    freq=\"T\",\n",
    "    primary_period=600,\n",
    "    secondary_period=3000,\n",
    "    seasonal_ts_prob=1.0,\n",
    "    baseline_range=(200.0, 350.0),\n",
    "    slope_range=(-0.001, 0.01),\n",
    "    amplitude_range=(10, 75),\n",
    "    cosine_ratio_range=(0.5, 0.9),\n",
    "    noise_range=(5, 15),\n",
    ")\n",
    "\n",
    "# shape: (9000, 1) with column names [s1]\n",
    "ts_df = ts_generator.gen_tseries()  "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "source": [
    "ts_df.plot(title=\"Time series data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the generated data into train and test dataframes, with test set size 1000, validation set size as 1000, leaving 7000 datapoints for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "source": [
    "train_df, test_df = ts_generator.train_test_split(ts_df, test_size=TEST_SIZE)\n",
    "train_df, val_df = ts_generator.train_test_split(train_df, test_size=VALIDATION_SIZE)\n",
    "print(f\"train shape: {train_df.shape}\\nval shape: {val_df.shape}\\ntest shape: {test_df.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "source": [
    "ax = train_df[\"s1\"].plot(label=\"train\")\n",
    "val_df[\"s1\"].plot(ax=ax, label=\"val\")\n",
    "test_df[\"s1\"].plot(ax=ax, label=\"test\", title=\"Time series before injecting anomalies\")\n",
    "plt.legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Inject anomalies:\n",
    "\n",
    "Now, we inject anomalies into the test set using the `AnomalyGenerator`. Here we are adding `contextual` anomalies, you could also try `causal` or `collective` anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "source": [
    "from numalogic.synthetic import AnomalyGenerator\n",
    "\n",
    "anomaly_generator = AnomalyGenerator(train_df, anomaly_type=\"contextual\", anomaly_ratio=0.2)\n",
    "outliers_test_df = anomaly_generator.inject_anomalies(\n",
    "    test_df, cols=[\"s1\"], impact=1\n",
    ")\n",
    "outliers_test_df.drop(columns = \"is_anomaly\", inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ax = train_df[\"s1\"].plot(label=\"train\")\n",
    "val_df[\"s1\"].plot(ax=ax, label=\"val\")\n",
    "outliers_test_df[\"s1\"].plot(ax=ax, label=\"test\", title=\"Time series after injecting anomalies\")\n",
    "plt.legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing:\n",
    "\n",
    "Pre-processing is an optional step, where we can clean, scale or transform the data. We can transform the features by scaling each feature to a given range, while keeping the shape of the distribution same. \n",
    "\n",
    "Here we are using `TanhScaler` from numalogic, you could also try other scikit-learn scalers like `StandarScaler` or `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "source": [
    "from numalogic.transforms import TanhScaler\n",
    "\n",
    "scaler = TanhScaler()\n",
    "X_train = scaler.fit_transform(train_df.to_numpy())\n",
    "X_val = scaler.transform(val_df.to_numpy())\n",
    "X_test = scaler.transform(outliers_test_df.to_numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training step, we define the model and train it on the training data set.\n",
    "Let's use simple 1D convolutional autoencoder for this pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from numalogic.models.autoencoder import TimeseriesTrainer\n",
    "from numalogic.models.autoencoder.variants import Conv1dAE\n",
    "from numalogic.tools.data import StreamingDataset\n",
    "\n",
    "SEQ_LEN = 24  # length of the sequence\n",
    "MAX_EPOCHS = 30  # number of epochs to run\n",
    "BATCH_SIZE = 64  # training batch size\n",
    "\n",
    "model = Conv1dAE(seq_len=SEQ_LEN, in_channels=1, enc_channels=(8, 4))\n",
    "model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "trainer = TimeseriesTrainer(max_epochs=MAX_EPOCHS, enable_progress_bar=True)\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=DataLoader(StreamingDataset(X_train, seq_len=SEQ_LEN), batch_size=BATCH_SIZE),\n",
    "    val_dataloaders=DataLoader(StreamingDataset(X_val, seq_len=SEQ_LEN), batch_size=BATCH_SIZE)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Threshold calculation:\n",
    "\n",
    "After training the main model, we need to perform a threshold calculation. The autoencoder tries to encode the representation of the normal input data,\n",
    "and tries to reconstuct the output. The difference between the actual input and the reconstructed output is what we call as the reconstruction error.\n",
    "\n",
    "Some amount of recconstruction error is normal, and we need to know what amount is normal, and what can be called as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate training set reconstruction error\n",
    "train_reconerr = trainer.predict(model, dataloaders=DataLoader(StreamingDataset(X_train, seq_len=SEQ_LEN)))\n",
    "print(train_reconerr.shape)\n",
    "plt.plot(train_reconerr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate validation set reconstruction error\n",
    "val_reconerr = trainer.predict(model, dataloaders=DataLoader(StreamingDataset(X_val, seq_len=SEQ_LEN)))\n",
    "print(val_reconerr.shape)\n",
    "plt.plot(val_reconerr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we say that anything more than 3 times the standard deviation from the mean is anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate the threshold\n",
    "from numalogic.models.threshold import StdDevThreshold\n",
    "\n",
    "threshold_clf = StdDevThreshold(std_factor=3.0)\n",
    "_ = threshold_clf.fit(val_reconerr.numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference:\n",
    "\n",
    "Now, we use the trained model from above to predict anomalies in the test data. The predict method return the reconstruction error produced by the model. \n",
    "\n",
    "Score method returns the anomaly score, calculated using thresholds. If the score is less than 1, it indicates an inlier. And the score greater than 1 indicates an outlier or anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "source": [
    "# Get the reconstruction error on the test set\n",
    "test_recon_err = trainer.predict(model, dataloaders=DataLoader(StreamingDataset(X_test, seq_len=SEQ_LEN)))\n",
    "print(test_recon_err.shape)\n",
    "plt.plot(test_recon_err)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Get the anomaly score using the threshold estimator.\n",
    "test_anomaly_score = threshold_clf.score_samples(test_recon_err.numpy())\n",
    "print(test_anomaly_score.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "source": [
    "# Convert the score array into a dataframe\n",
    "anomalies_df = pd.DataFrame(data=test_anomaly_score, columns=outliers_test_df.columns, index=outliers_test_df.index)\n",
    "anomalies_df.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "source": [
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(outliers_test_df[\"s1\"])\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(anomalies_df[\"s1\"], color=\"orange\")\n",
    "plt.legend([l1, l2], [\"test data\", \"anomaly score\"])\n",
    "plt.title(\"S1 data vs anomaly score\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7. Post-processing:\n",
    "Post-processing step is an optional step, where we normalize the anomalies between 0-10. This can make the scores more human interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from numalogic.transforms import TanhNorm\n",
    "\n",
    "# scale factor makes sure that the scores are normalized between 0 and 10\n",
    "postproc_clf = TanhNorm(scale_factor=10)\n",
    "test_anomaly_score_norm = postproc_clf.fit_transform(test_anomaly_score)\n",
    "print(test_anomaly_score_norm.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "source": [
    "norm_anomalies_df = pd.DataFrame(data=test_anomaly_score_norm, columns=outliers_test_df.columns, index=outliers_test_df.index)\n",
    "norm_anomalies_df.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "source": [
    "f, ax1 = plt.subplots(1, 1, figsize=(12, 4))\n",
    "(l1,) = ax1.plot(outliers_test_df[\"s1\"])\n",
    "ax2 = ax1.twinx()\n",
    "(l2,) = ax2.plot(norm_anomalies_df[\"s1\"], color=\"orange\")\n",
    "l3 = ax1.plot(test_df[\"s1\"], color=\"g\")\n",
    "plt.legend([l1, l2, l3], [\"test data with outlier\", \"anomaly score\", \"test data\"])\n",
    "plt.title(\"S1 data vs normalized anomaly score\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "l1, = axes[0].plot(outliers_test_df[\"s1\"], label=\"test data with outlier\")\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"S1 data vs normalized anomaly score\")\n",
    "l2, = axes[1].plot(norm_anomalies_df[\"s1\"], color=\"orange\", label=\"norm anomaly score\")\n",
    "axes[1].legend()\n",
    "l3 = axes[2].plot(test_df[\"s1\"], color=\"g\", label=\"test data without outlier\")\n",
    "axes[2].legend()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "# LSTM sentiment classifier\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
