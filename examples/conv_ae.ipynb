{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%load_ext autoreload\n",
    "%load_ext nb_black\n",
    "%load_ext autotime\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 2]\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a synthetic time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "from numalogic.synthetic import SyntheticTSGenerator\n",
    "from numalogic.synthetic import AnomalyGenerator\n",
    "\n",
    "ts_generator = SyntheticTSGenerator(\n",
    "    seq_len=12000,\n",
    "    num_series=1,\n",
    "    freq=\"T\",\n",
    "    primary_period=480,\n",
    "    secondary_period=3000,\n",
    "    seasonal_ts_prob=1.0,\n",
    "    baseline_range=(200.0, 350.0),\n",
    "    slope_range=(-0.001, 0.01),\n",
    "    amplitude_range=(10, 75),\n",
    "    cosine_ratio_range=(0.5, 0.9),\n",
    "    noise_range=(10, 15),\n",
    ")\n",
    "ts_df = ts_generator.gen_tseries()\n",
    "ts_df.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "train_df, test_df = ts_generator.train_test_split(ts_df, test_size=2400)\n",
    "train_df, val_df = ts_generator.train_test_split(train_df, test_size=2000)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject synthetic anomalies in the test data\n",
    "Let's generate a contextual anomaly with a low impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "anomaly_generator = AnomalyGenerator(\n",
    "    train_df, anomaly_type=\"contextual\", anomaly_ratio=0.2\n",
    ")\n",
    "outlier_test_df = anomaly_generator.inject_anomalies(test_df, cols=[\"s1\"], impact=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "outlier_test_df.rename(columns={\"s1\": \"s1_anomaly\"}, inplace=True)\n",
    "fig, ax = plt.subplots()\n",
    "outlier_test_df[[\"s1_anomaly\"]].plot(ax=ax)\n",
    "test_df.plot(ax=ax)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "We will use the Tanh scaler here from numalogic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "from numalogic.transforms import TanhScaler\n",
    "\n",
    "scaler = TanhScaler()\n",
    "x_train = scaler.fit_transform(train_df.to_numpy())\n",
    "x_val = scaler.transform(val_df.to_numpy())\n",
    "x_test = scaler.transform(outlier_test_df[[\"s1_anomaly\"]].to_numpy())\n",
    "x_test_good = scaler.transform(test_df.to_numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model training parameters\n",
    "First, we will prepare some torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "from numalogic.tools.data import StreamingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "SEQ_LEN = 12\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 40\n",
    "\n",
    "train_dataset = StreamingDataset(x_train, seq_len=SEQ_LEN)\n",
    "val_dataset = StreamingDataset(x_val, seq_len=SEQ_LEN)\n",
    "test_dataset = StreamingDataset(x_test, seq_len=SEQ_LEN)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's define 1st convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "from numalogic.models.autoencoder.variants import Conv1dAE\n",
    "from numalogic.models.autoencoder import TimeseriesTrainer\n",
    "\n",
    "model_1 = Conv1dAE(seq_len=SEQ_LEN, in_channels=1, enc_channels=(16, 32, 8), enc_kernel_sizes=3)\n",
    "print(model_1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "trainer = TimeseriesTrainer(max_epochs=MAX_EPOCHS, enable_progress_bar=True)\n",
    "trainer.fit(model_1, train_dataloaders=DataLoader(train_dataset, batch_size=BATCH_SIZE))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the reconstruction error for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "train_reconerr_1 = trainer.predict(\n",
    "    model_1, dataloaders=DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()\n",
    "val_reconerr_1 = trainer.predict(\n",
    "    model_1, dataloaders=DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()\n",
    "test_reconerr_1 = trainer.predict(\n",
    "    model_1, dataloaders=DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "fig = plt.plot(train_reconerr_1, color=\"g\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "fig = plt.plot(val_reconerr_1, color=\"g\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "axes[0].set_title(\"Test data reconstruction error\")\n",
    "axes[1].set_title(\"Scaled anomalous test data\")\n",
    "_ = axes[0].plot(test_reconerr_1, color=\"g\")\n",
    "_ = axes[1].plot(x_test, label=\"anomalous\")\n",
    "_ = axes[1].plot(x_test_good, label=\"original\")\n",
    "plt.legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model 2\n",
    "Using a 2 conv layer network in the encoder and decoder, along with sigmoid activation function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model_2 = Conv1dAE(seq_len=SEQ_LEN, in_channels=1, enc_channels=(16, 8), enc_kernel_sizes=(5, 3), dec_activation=\"sigmoid\")\n",
    "model_2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "trainer = TimeseriesTrainer(max_epochs=MAX_EPOCHS, enable_progress_bar=True)\n",
    "trainer.fit(model_2, train_dataloaders=DataLoader(train_dataset, batch_size=BATCH_SIZE))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_reconerr_2 = trainer.predict(\n",
    "    model_2, dataloaders=DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()\n",
    "val_reconerr_2 = trainer.predict(\n",
    "    model_2, dataloaders=DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()\n",
    "test_reconerr_2 = trainer.predict(\n",
    "    model_2, dataloaders=DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig = plt.plot(train_reconerr_2, color=\"g\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig = plt.plot(val_reconerr_2, color=\"g\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "axes[0].set_title(\"Test data reconstruction error\")\n",
    "axes[1].set_title(\"Scaled anomalous test data\")\n",
    "_ = axes[0].plot(test_reconerr_2, color=\"g\")\n",
    "_ = axes[1].plot(x_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model 3\n",
    "Using a similar architecture as model 2, but changing the number of filters/channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# with upsampling\n",
    "model_3 = Conv1dAE(seq_len=SEQ_LEN, in_channels=1, enc_channels=[32, 4], enc_kernel_sizes=3)\n",
    "model_3"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "trainer = TimeseriesTrainer(accelerator=\"cpu\", max_epochs=MAX_EPOCHS, enable_progress_bar=True)\n",
    "trainer.fit(model_3, train_dataloaders=DataLoader(StreamingDataset(x_train, seq_len=SEQ_LEN), batch_size=BATCH_SIZE))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_reconerr_3 = trainer.predict(\n",
    "    model_3, dataloaders=DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()\n",
    "val_reconerr_3 = trainer.predict(\n",
    "    model_3, dataloaders=DataLoader(val_dataset,\n",
    "                                    batch_size=BATCH_SIZE)\n",
    ").numpy()\n",
    "test_reconerr_3 = trainer.predict(\n",
    "    model_3, dataloaders=DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    ").numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig = plt.plot(train_reconerr_3, color=\"g\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig = plt.plot(val_reconerr_3, color=\"g\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "axes[0].set_title(\"Test data reconstruction error\")\n",
    "# axes[0].set_ylim(0, 0.5)\n",
    "axes[1].set_title(\"Scaled anomalous test data\")\n",
    "_ = axes[0].plot(test_reconerr_3, color=\"g\")\n",
    "_ = axes[1].plot(x_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Comparing the test reconstructions for the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_reconerr_1, label=\"Model 1\")\n",
    "ax.plot(test_reconerr_2, label=\"Model 2\")\n",
    "ax.plot(test_reconerr_3, label=\"Model 3\")\n",
    "plt.legend()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
