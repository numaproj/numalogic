{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Numalogic \u00b6 Numalogic is a collection of ML models and algorithms for real-time data analytics and AIOps including anomaly detection. Numalogic can be installed as a library and used to build end-to-end ML pipelines. For streaming real time data processing, it can also be paired with our steaming data platform Numaflow . Key Features \u00b6 Ease of use: simple and efficient tools for predictive data analytics Reusability: all the functionalities can be re-used in various contexts Model selection: easy to compare, validate, fine-tune and choose the model that works best with each data set Data processing: readily available feature extraction, scaling, transforming and normalization tools Extensibility: adding your own functions or extending over the existing capabilities Model Storage: out-of-the-box support for MLFlow and support for other model ML lifecycle management tools Use Cases \u00b6 Deployment failure detection System failure detection for node failures or crashes Fraud detection Network intrusion detection Forecasting on time series data Getting Started \u00b6 For set-up information and running your first pipeline using numalogic, please see our getting started guide .","title":"Home"},{"location":"#numalogic","text":"Numalogic is a collection of ML models and algorithms for real-time data analytics and AIOps including anomaly detection. Numalogic can be installed as a library and used to build end-to-end ML pipelines. For streaming real time data processing, it can also be paired with our steaming data platform Numaflow .","title":"Numalogic"},{"location":"#key-features","text":"Ease of use: simple and efficient tools for predictive data analytics Reusability: all the functionalities can be re-used in various contexts Model selection: easy to compare, validate, fine-tune and choose the model that works best with each data set Data processing: readily available feature extraction, scaling, transforming and normalization tools Extensibility: adding your own functions or extending over the existing capabilities Model Storage: out-of-the-box support for MLFlow and support for other model ML lifecycle management tools","title":"Key Features"},{"location":"#use-cases","text":"Deployment failure detection System failure detection for node failures or crashes Fraud detection Network intrusion detection Forecasting on time series data","title":"Use Cases"},{"location":"#getting-started","text":"For set-up information and running your first pipeline using numalogic, please see our getting started guide .","title":"Getting Started"},{"location":"autoencoders/","text":"Autoencoders \u00b6 An Autoencoder is a type of Artificial Neural Network, used to learn efficient data representations (encoding) of unlabeled data. It mainly consists of 2 components: an encoder and a decoder. The encoder compresses the input into a lower dimensional code, the decoder then reconstructs the input only using this code. Datamodules \u00b6 Pytorch-lightning datamodules abstracts and separates the data functionality from the model and training itself. Numalogic provides TimeseriesDataModule to help set up and load dataloaders. import numpy as np from numalogic.tools.data import TimeseriesDataModule train_data = np . random . randn ( 100 , 3 ) datamodule = TimeseriesDataModule ( 12 , train_data , batch_size = 128 ) Autoencoder Trainer \u00b6 Numalogic provides a subclass of Pytorch-Lightning Trainer module specifically for Autoencoders. This trainer provides a mechanism to train, validate and infer on data, with all the parameters supported by Lightning Trainer. Here we are using VanillaAE , a Vanilla Autoencoder model. from numalogic.models.autoencoder.variants import VanillaAE from numalogic.models.autoencoder import TimeseriesTrainer model = VanillaAE ( seq_len = 12 , n_features = 3 ) trainer = TimeseriesTrainer ( max_epochs = 50 , enable_progress_bar = True ) trainer . fit ( model , datamodule = datamodule ) Autoencoder Variants \u00b6 Numalogic supports 2 variants of Autoencoders currently. More details can be found here . 1. Autoencoders \u00b6 Basic autoencoders aim to find representations of the input data in a latent dimensional space. Ideally, in order for the network to learn meaningful patterns, it is recommended that undercomplete architectures are used, i.e. the latent space dimension being less than the input dimension. Examples would be VanillaAE , Conv1dAE , LSTMAE and TransformerAE 2. Sparse autoencoders \u00b6 A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer. So, by adding a sparsity regularization, we will be able to stop the neural network from copying the input and reduce overfitting. Examples would be SparseVanillaAE , SparseConv1dAE , SparseLSTMAE and SparseTransformerAE Network architectures \u00b6 Numalogic currently supports the following architectures. Fully Connected \u00b6 Vanilla Autoencoder model comprising only fully connected layers. from numalogic.models.autoencoder.variants import VanillaAE model = VanillaAE ( seq_len = 12 , n_features = 2 ) Convolutional \u00b6 Conv1dAE is a 1D convolutional autoencoder. The encoder network consists of convolutional layers and max pooling layers. The decoder network tries to reconstruct the same input shape by corresponding transposed convolutional and upsampling layers. from numalogic.models.autoencoder.variants import SparseConv1dAE model = SparseConv1dAE ( beta = 1e-2 , seq_len = 12 , in_channels = 3 , enc_channels = [ 8 , 4 ]) LSTM \u00b6 An LSTM (Long Short-Term Memory) Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. from numalogic.models.autoencoder.variants import LSTMAE model = LSTMAE ( seq_len = 12 , no_features = 2 , embedding_dim = 15 ) Transformer \u00b6 The transformer-based Autoencoder model was inspired from Attention is all you need paper. It consists of an encoder and a decoder which are both stacks of residual attention blocks, i.e a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. These blocks can process an input sequence of variable length n without exhibiting a recurrent structure and allows transformer-based encoder-decoders to be highly parallelizable. from numalogic.models.autoencoder.variants import TransformerAE model = TransformerAE ( num_heads = 8 , seq_length = 12 , dim_feedforward = 64 , num_encoder_layers = 3 , num_decoder_layers = 1 , )","title":"Autoencoders"},{"location":"autoencoders/#autoencoders","text":"An Autoencoder is a type of Artificial Neural Network, used to learn efficient data representations (encoding) of unlabeled data. It mainly consists of 2 components: an encoder and a decoder. The encoder compresses the input into a lower dimensional code, the decoder then reconstructs the input only using this code.","title":"Autoencoders"},{"location":"autoencoders/#datamodules","text":"Pytorch-lightning datamodules abstracts and separates the data functionality from the model and training itself. Numalogic provides TimeseriesDataModule to help set up and load dataloaders. import numpy as np from numalogic.tools.data import TimeseriesDataModule train_data = np . random . randn ( 100 , 3 ) datamodule = TimeseriesDataModule ( 12 , train_data , batch_size = 128 )","title":"Datamodules"},{"location":"autoencoders/#autoencoder-trainer","text":"Numalogic provides a subclass of Pytorch-Lightning Trainer module specifically for Autoencoders. This trainer provides a mechanism to train, validate and infer on data, with all the parameters supported by Lightning Trainer. Here we are using VanillaAE , a Vanilla Autoencoder model. from numalogic.models.autoencoder.variants import VanillaAE from numalogic.models.autoencoder import TimeseriesTrainer model = VanillaAE ( seq_len = 12 , n_features = 3 ) trainer = TimeseriesTrainer ( max_epochs = 50 , enable_progress_bar = True ) trainer . fit ( model , datamodule = datamodule )","title":"Autoencoder Trainer"},{"location":"autoencoders/#autoencoder-variants","text":"Numalogic supports 2 variants of Autoencoders currently. More details can be found here .","title":"Autoencoder Variants"},{"location":"autoencoders/#1-autoencoders","text":"Basic autoencoders aim to find representations of the input data in a latent dimensional space. Ideally, in order for the network to learn meaningful patterns, it is recommended that undercomplete architectures are used, i.e. the latent space dimension being less than the input dimension. Examples would be VanillaAE , Conv1dAE , LSTMAE and TransformerAE","title":"1. Autoencoders"},{"location":"autoencoders/#2-sparse-autoencoders","text":"A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer. So, by adding a sparsity regularization, we will be able to stop the neural network from copying the input and reduce overfitting. Examples would be SparseVanillaAE , SparseConv1dAE , SparseLSTMAE and SparseTransformerAE","title":"2. Sparse autoencoders"},{"location":"autoencoders/#network-architectures","text":"Numalogic currently supports the following architectures.","title":"Network architectures"},{"location":"autoencoders/#fully-connected","text":"Vanilla Autoencoder model comprising only fully connected layers. from numalogic.models.autoencoder.variants import VanillaAE model = VanillaAE ( seq_len = 12 , n_features = 2 )","title":"Fully Connected"},{"location":"autoencoders/#convolutional","text":"Conv1dAE is a 1D convolutional autoencoder. The encoder network consists of convolutional layers and max pooling layers. The decoder network tries to reconstruct the same input shape by corresponding transposed convolutional and upsampling layers. from numalogic.models.autoencoder.variants import SparseConv1dAE model = SparseConv1dAE ( beta = 1e-2 , seq_len = 12 , in_channels = 3 , enc_channels = [ 8 , 4 ])","title":"Convolutional"},{"location":"autoencoders/#lstm","text":"An LSTM (Long Short-Term Memory) Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. from numalogic.models.autoencoder.variants import LSTMAE model = LSTMAE ( seq_len = 12 , no_features = 2 , embedding_dim = 15 )","title":"LSTM"},{"location":"autoencoders/#transformer","text":"The transformer-based Autoencoder model was inspired from Attention is all you need paper. It consists of an encoder and a decoder which are both stacks of residual attention blocks, i.e a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. These blocks can process an input sequence of variable length n without exhibiting a recurrent structure and allows transformer-based encoder-decoders to be highly parallelizable. from numalogic.models.autoencoder.variants import TransformerAE model = TransformerAE ( num_heads = 8 , seq_length = 12 , dim_feedforward = 64 , num_encoder_layers = 3 , num_decoder_layers = 1 , )","title":"Transformer"},{"location":"data-generator/","text":"Data Generator \u00b6 Numalogic provides a data generator to create some synthetic time series data, that can be used as train or test data sets. Using the synthetic data, we can: Compare and evaluate different ML algorithms, since we have labeled anomalies Understand different types of anomalies, and our models' performance on each of them Recreate realtime scenarios Generate multivariate timeseries \u00b6 from numalogic.synthetic import SyntheticTSGenerator ts_generator = SyntheticTSGenerator ( seq_len = 8000 , num_series = 3 , freq = \"T\" , primary_period = 720 , secondary_period = 6000 , seasonal_ts_prob = 0.8 , baseline_range = ( 200.0 , 350.0 ), slope_range = ( - 0.001 , 0.01 ), amplitude_range = ( 10 , 75 ), cosine_ratio_range = ( 0.5 , 0.9 ), noise_range = ( 5 , 15 ), ) # shape: (8000, 3) with column names [s1, s2, s3] ts_df = ts_generator . gen_tseries () # Split into test and train train_df , test_df = ts_generator . train_test_split ( ts_df , test_size = 1000 ) Inject anomalies \u00b6 Now, once we generate the synthetic data like above, we can inject anomalies into the test data set using AnomalyGenerator . AnomalyGenerator supports the following types of anomalies: global: Outliers in the global context contextual: Outliers only in the seasonal context causal: Outliers caused by a temporal causal effect collective: Outliers present simultaneously in two or more time series You can also use anomaly_ratio to adjust the ratio of anomalous data points wrt number of samples. from numalogic.synthetic import AnomalyGenerator # columns to inject anomalies injected_cols = [ \"s1\" , \"s2\" ] anomaly_generator = AnomalyGenerator ( train_df , anomaly_type = \"contextual\" , anomaly_ratio = 0.3 ) outlier_test_df = anomaly_generator . inject_anomalies ( test_df , cols = injected_cols , impact = 1.5 )","title":"Data Generator"},{"location":"data-generator/#data-generator","text":"Numalogic provides a data generator to create some synthetic time series data, that can be used as train or test data sets. Using the synthetic data, we can: Compare and evaluate different ML algorithms, since we have labeled anomalies Understand different types of anomalies, and our models' performance on each of them Recreate realtime scenarios","title":"Data Generator"},{"location":"data-generator/#generate-multivariate-timeseries","text":"from numalogic.synthetic import SyntheticTSGenerator ts_generator = SyntheticTSGenerator ( seq_len = 8000 , num_series = 3 , freq = \"T\" , primary_period = 720 , secondary_period = 6000 , seasonal_ts_prob = 0.8 , baseline_range = ( 200.0 , 350.0 ), slope_range = ( - 0.001 , 0.01 ), amplitude_range = ( 10 , 75 ), cosine_ratio_range = ( 0.5 , 0.9 ), noise_range = ( 5 , 15 ), ) # shape: (8000, 3) with column names [s1, s2, s3] ts_df = ts_generator . gen_tseries () # Split into test and train train_df , test_df = ts_generator . train_test_split ( ts_df , test_size = 1000 )","title":"Generate multivariate timeseries"},{"location":"data-generator/#inject-anomalies","text":"Now, once we generate the synthetic data like above, we can inject anomalies into the test data set using AnomalyGenerator . AnomalyGenerator supports the following types of anomalies: global: Outliers in the global context contextual: Outliers only in the seasonal context causal: Outliers caused by a temporal causal effect collective: Outliers present simultaneously in two or more time series You can also use anomaly_ratio to adjust the ratio of anomalous data points wrt number of samples. from numalogic.synthetic import AnomalyGenerator # columns to inject anomalies injected_cols = [ \"s1\" , \"s2\" ] anomaly_generator = AnomalyGenerator ( train_df , anomaly_type = \"contextual\" , anomaly_ratio = 0.3 ) outlier_test_df = anomaly_generator . inject_anomalies ( test_df , cols = injected_cols , impact = 1.5 )","title":"Inject anomalies"},{"location":"forecasting/","text":"Forcasting \u00b6 Numalogic supports the following variants of forecasting based anomaly detection models. Naive Forecasters \u00b6 Baseline Forecaster \u00b6 This is a naive forecaster, that uses a combination of: Log transformation Z-Score normalization from numalogic.models.forecast.variants import BaselineForecaster model = BaselineForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df ) Seasonal Naive Forecaster \u00b6 A naive forecaster that takes seasonality into consideration and predicts the previous day/week values. from numalogic.models.forecast.variants import SeasonalNaiveForecaster model = SeasonalNaiveForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df )","title":"Forcasting"},{"location":"forecasting/#forcasting","text":"Numalogic supports the following variants of forecasting based anomaly detection models.","title":"Forcasting"},{"location":"forecasting/#naive-forecasters","text":"","title":"Naive Forecasters"},{"location":"forecasting/#baseline-forecaster","text":"This is a naive forecaster, that uses a combination of: Log transformation Z-Score normalization from numalogic.models.forecast.variants import BaselineForecaster model = BaselineForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df )","title":"Baseline Forecaster"},{"location":"forecasting/#seasonal-naive-forecaster","text":"A naive forecaster that takes seasonality into consideration and predicts the previous day/week values. from numalogic.models.forecast.variants import SeasonalNaiveForecaster model = SeasonalNaiveForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df )","title":"Seasonal Naive Forecaster"},{"location":"inference/","text":"Inference \u00b6 Now, once we have the model trained using one of the ML algorthims, we can predict the anomalies in the test data. This can be a streaming or a batched data. X_test = scaler . transform ( outlier_test_df . to_numpy ()) # predict method returns the reconstruction produced by the AE test_recon = model . predict ( X_test ) # score method returns the anomaly score, calculated using thresholds. # A number less than 1 indicates an inlier, and greater than 1 indicates an outlier. test_anomaly_score = model . score ( X_test )","title":"Inference"},{"location":"inference/#inference","text":"Now, once we have the model trained using one of the ML algorthims, we can predict the anomalies in the test data. This can be a streaming or a batched data. X_test = scaler . transform ( outlier_test_df . to_numpy ()) # predict method returns the reconstruction produced by the AE test_recon = model . predict ( X_test ) # score method returns the anomaly score, calculated using thresholds. # A number less than 1 indicates an inlier, and greater than 1 indicates an outlier. test_anomaly_score = model . score ( X_test )","title":"Inference"},{"location":"ml-flow/","text":"MLflow \u00b6 Numalogic has built in support for Mlflow's tracking and logging system. Starting MLflow \u00b6 To start the mlflow server on localhost , which has already been installed optionally via poetry dependency, run the following command. Replace the {directory} with the path you want to save the models. mlflow server \\ --default-artifact-root { directory } /mlruns --serve-artifacts \\ --backend-store-uri sqlite:///mlflow.db --host 0 .0.0.0 --port 5000 Once the mlflow server has been started, you can navigate to http://127.0.0.1:5000/ to explore mlflow UI. Model saving \u00b6 Numalogic provides MLflowRegistry , to save and load models to/from MLflow. Here, tracking_uri is the uri where mlflow server is running. The static_keys and dynamic_keys are used to form a unique key for the model. The artifact would be the model or transformer object that needs to be saved. A dictionary of metadata can also be saved along with the artifact. from numalogic.registry import MLflowRegistry from numalogic.models.autoencoder.variants import VanillaAE model = VanillaAE ( seq_len = 10 ) # static and dynamic keys are used to look up a model static_keys = [ \"model\" , \"autoencoder\" ] dynamic_keys = [ \"vanilla\" , \"seq10\" ] registry = MLflowRegistry ( tracking_uri = \"http://0.0.0.0:5000\" ) registry . save ( skeys = static_keys , dkeys = dynamic_keys , artifact = model , seq_len = 10 , lr = 0.001 ) Model loading \u00b6 Once, the models are save to MLflow, the load function of MLflowRegistry can be used to load the model. from numalogic.registry import MLflowRegistry static_keys = [ \"model\" , \"autoencoder\" ] dynamic_keys = [ \"vanilla\" , \"seq10\" ] registry = MLflowRegistry ( tracking_uri = \"http://0.0.0.0:8080\" ) artifact_data = registry . load ( skeys = static_keys , dkeys = dynamic_keys , artifact_type = \"pytorch\" ) # get the model and metadata model = artifact_data . artifact model_metadata = artifact_data . metadata For more details, please refer to MLflow Model Registry","title":"MLflow"},{"location":"ml-flow/#mlflow","text":"Numalogic has built in support for Mlflow's tracking and logging system.","title":"MLflow"},{"location":"ml-flow/#starting-mlflow","text":"To start the mlflow server on localhost , which has already been installed optionally via poetry dependency, run the following command. Replace the {directory} with the path you want to save the models. mlflow server \\ --default-artifact-root { directory } /mlruns --serve-artifacts \\ --backend-store-uri sqlite:///mlflow.db --host 0 .0.0.0 --port 5000 Once the mlflow server has been started, you can navigate to http://127.0.0.1:5000/ to explore mlflow UI.","title":"Starting MLflow"},{"location":"ml-flow/#model-saving","text":"Numalogic provides MLflowRegistry , to save and load models to/from MLflow. Here, tracking_uri is the uri where mlflow server is running. The static_keys and dynamic_keys are used to form a unique key for the model. The artifact would be the model or transformer object that needs to be saved. A dictionary of metadata can also be saved along with the artifact. from numalogic.registry import MLflowRegistry from numalogic.models.autoencoder.variants import VanillaAE model = VanillaAE ( seq_len = 10 ) # static and dynamic keys are used to look up a model static_keys = [ \"model\" , \"autoencoder\" ] dynamic_keys = [ \"vanilla\" , \"seq10\" ] registry = MLflowRegistry ( tracking_uri = \"http://0.0.0.0:5000\" ) registry . save ( skeys = static_keys , dkeys = dynamic_keys , artifact = model , seq_len = 10 , lr = 0.001 )","title":"Model saving"},{"location":"ml-flow/#model-loading","text":"Once, the models are save to MLflow, the load function of MLflowRegistry can be used to load the model. from numalogic.registry import MLflowRegistry static_keys = [ \"model\" , \"autoencoder\" ] dynamic_keys = [ \"vanilla\" , \"seq10\" ] registry = MLflowRegistry ( tracking_uri = \"http://0.0.0.0:8080\" ) artifact_data = registry . load ( skeys = static_keys , dkeys = dynamic_keys , artifact_type = \"pytorch\" ) # get the model and metadata model = artifact_data . artifact model_metadata = artifact_data . metadata For more details, please refer to MLflow Model Registry","title":"Model loading"},{"location":"post-processing/","text":"Post Processing \u00b6 After the raw scores have been generated, we might need to do some additional postprocessing, for various reasons. Tanh Score Normalization \u00b6 Tanh normalization step is an optional step, where we normalize the anomalies between 0-10. This is mostly to make the scores more understandable. import numpy as np from numalogic.transforms import tanh_norm raw_anomaly_score = np . random . randn ( 10 , 2 ) test_anomaly_score_norm = tanh_norm ( raw_anomaly_score ) A scikit-learn compatible API is also available. import numpy as np from numalogic.transforms import TanhNorm raw_score = np . random . randn ( 10 , 2 ) norm = TanhNorm ( scale_factor = 10 , smooth_factor = 10 ) norm_score = norm . fit_transform ( raw_score ) Exponentially Weighted Moving Average \u00b6 The Exponentially Weighted Moving Average (EWMA) serves as an effective smoothing function, emphasizing the importance of more recent anomaly scores over those of previous elements within a sliding window. This approach proves particularly beneficial in streaming inference scenarios, as it allows for earlier increases in anomaly scores when a new outlier data point is encountered. Consequently, the EMA enables a more responsive and dynamic assessment of streaming data, facilitating timely detection and response to potential anomalies. import numpy as np from numalogic.transforms import ExpMovingAverage raw_score = np . array ([ 1.0 , 1.5 , 1.2 , 3.5 , 2.7 , 5.6 , 7.1 , 6.9 , 4.2 , 1.1 ]) . reshape ( - 1 , 1 ) postproc_clf = ExpMovingAverage ( beta = 0.5 ) out = postproc_clf . transform ( raw_score ) # out: [[1.3], [1.433], [1.333], [2.473], [2.591], [4.119], [5.621], [6.263], [5.229], [3.163]]","title":"Post Processing"},{"location":"post-processing/#post-processing","text":"After the raw scores have been generated, we might need to do some additional postprocessing, for various reasons.","title":"Post Processing"},{"location":"post-processing/#tanh-score-normalization","text":"Tanh normalization step is an optional step, where we normalize the anomalies between 0-10. This is mostly to make the scores more understandable. import numpy as np from numalogic.transforms import tanh_norm raw_anomaly_score = np . random . randn ( 10 , 2 ) test_anomaly_score_norm = tanh_norm ( raw_anomaly_score ) A scikit-learn compatible API is also available. import numpy as np from numalogic.transforms import TanhNorm raw_score = np . random . randn ( 10 , 2 ) norm = TanhNorm ( scale_factor = 10 , smooth_factor = 10 ) norm_score = norm . fit_transform ( raw_score )","title":"Tanh Score Normalization"},{"location":"post-processing/#exponentially-weighted-moving-average","text":"The Exponentially Weighted Moving Average (EWMA) serves as an effective smoothing function, emphasizing the importance of more recent anomaly scores over those of previous elements within a sliding window. This approach proves particularly beneficial in streaming inference scenarios, as it allows for earlier increases in anomaly scores when a new outlier data point is encountered. Consequently, the EMA enables a more responsive and dynamic assessment of streaming data, facilitating timely detection and response to potential anomalies. import numpy as np from numalogic.transforms import ExpMovingAverage raw_score = np . array ([ 1.0 , 1.5 , 1.2 , 3.5 , 2.7 , 5.6 , 7.1 , 6.9 , 4.2 , 1.1 ]) . reshape ( - 1 , 1 ) postproc_clf = ExpMovingAverage ( beta = 0.5 ) out = postproc_clf . transform ( raw_score ) # out: [[1.3], [1.433], [1.333], [2.473], [2.591], [4.119], [5.621], [6.263], [5.229], [3.163]]","title":"Exponentially Weighted Moving Average"},{"location":"pre-processing/","text":"Pre Processing \u00b6 When creating a Machine Learning pipeline, data pre-processing plays a crucial role that takes in raw data and transforms it into a format that can be understood and analyzed by the ML Models. Generally, the majority of real-word datasets are incomplete, inconsistent or inaccurate (contains errors or outliers). Applying ML algorithms on this raw data would give inaccurate results, as they would fail to identify the underlying patterns effectively. Quality decisions must be based on quality data. Data Preprocessing is important to get this quality data, without which it would just be a Garbage In, Garbage Out scenario. Numalogic provides the following tranformers for pre-processing the training or testing data sets. You can also pair it with scalers like MinMaxScaler from scikit-learn pre-processing tools. Log Transformer \u00b6 Log transformation is a data transformation method in which it replaces each data point x with a log(x). Now, with add_factor , each data point x is converted to log(x + add_factor) Log transformation reduces the variance in some distributions, especially with large outliers. import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import MinMaxScaler from numalogic.transforms import LogTransformer # Generate some random train and test data x_train = np . random . randn ( 100 , 3 ) x_test = np . random . randn ( 20 , 3 ) transformer = LogTransformer ( add_factor = 4 ) scaler = MinMaxScaler () pipeline = make_pipeline ( transformer , scaler ) x_train_scaled = pipeline . fit_transform ( x_train ) X_test_scaled = pipeline . transform ( x_test ) Static Power Transformer \u00b6 Static Power Transformer converts each data point x to x n . When add_factor is provided, each data point x is converted to (x + add_factor) n import numpy as np from numalogic.transforms import StaticPowerTransformer # Generate some random train and test data x_train = np . random . randn ( 100 , 3 ) x_test = np . random . randn ( 20 , 3 ) transformer = StaticPowerTransformer ( n = 3 , add_factor = 2 ) # Since this transformer is stateless, we can just call transform() x_train_scaled = transformer . transform ( x_train ) X_test_scaled = transformer . transform ( x_test ) Tanh Scaler \u00b6 Tanh Scaler is a stateful estimator that applies tanh normalization to the Z-score, and scales the values between 0 and 1. This scaler is seen to be more efficient as well as robust to the effect of outliers in the data. import numpy as np from numalogic.transforms import TanhScaler # Generate some random train and test data x_train = np . random . randn ( 100 , 3 ) x_test = np . random . randn ( 20 , 3 ) scaler = TanhScaler () x_train_scaled = scaler . fit_transform ( x_train ) x_test_scaled = scaler . transform ( x_test )","title":"Pre Processing"},{"location":"pre-processing/#pre-processing","text":"When creating a Machine Learning pipeline, data pre-processing plays a crucial role that takes in raw data and transforms it into a format that can be understood and analyzed by the ML Models. Generally, the majority of real-word datasets are incomplete, inconsistent or inaccurate (contains errors or outliers). Applying ML algorithms on this raw data would give inaccurate results, as they would fail to identify the underlying patterns effectively. Quality decisions must be based on quality data. Data Preprocessing is important to get this quality data, without which it would just be a Garbage In, Garbage Out scenario. Numalogic provides the following tranformers for pre-processing the training or testing data sets. You can also pair it with scalers like MinMaxScaler from scikit-learn pre-processing tools.","title":"Pre Processing"},{"location":"pre-processing/#log-transformer","text":"Log transformation is a data transformation method in which it replaces each data point x with a log(x). Now, with add_factor , each data point x is converted to log(x + add_factor) Log transformation reduces the variance in some distributions, especially with large outliers. import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import MinMaxScaler from numalogic.transforms import LogTransformer # Generate some random train and test data x_train = np . random . randn ( 100 , 3 ) x_test = np . random . randn ( 20 , 3 ) transformer = LogTransformer ( add_factor = 4 ) scaler = MinMaxScaler () pipeline = make_pipeline ( transformer , scaler ) x_train_scaled = pipeline . fit_transform ( x_train ) X_test_scaled = pipeline . transform ( x_test )","title":"Log Transformer"},{"location":"pre-processing/#static-power-transformer","text":"Static Power Transformer converts each data point x to x n . When add_factor is provided, each data point x is converted to (x + add_factor) n import numpy as np from numalogic.transforms import StaticPowerTransformer # Generate some random train and test data x_train = np . random . randn ( 100 , 3 ) x_test = np . random . randn ( 20 , 3 ) transformer = StaticPowerTransformer ( n = 3 , add_factor = 2 ) # Since this transformer is stateless, we can just call transform() x_train_scaled = transformer . transform ( x_train ) X_test_scaled = transformer . transform ( x_test )","title":"Static Power Transformer"},{"location":"pre-processing/#tanh-scaler","text":"Tanh Scaler is a stateful estimator that applies tanh normalization to the Z-score, and scales the values between 0 and 1. This scaler is seen to be more efficient as well as robust to the effect of outliers in the data. import numpy as np from numalogic.transforms import TanhScaler # Generate some random train and test data x_train = np . random . randn ( 100 , 3 ) x_test = np . random . randn ( 20 , 3 ) scaler = TanhScaler () x_train_scaled = scaler . fit_transform ( x_train ) x_test_scaled = scaler . transform ( x_test )","title":"Tanh Scaler"},{"location":"quick-start/","text":"Quick Start \u00b6 Installation \u00b6 Install pytorch-lightning \u00b6 Numalogic needs PyTorch and PyTorch Lightning to work. You can install simply by: pip install pytorch-lightning Or you can install your platform specific torch version from their website. Install Numalogic \u00b6 pip install numalogic Numalogic as a Library \u00b6 Numalogic can be used as an independent library, and it provides various ML models and tools. Here, we are using the TimeseriesTrainer . Refer to training section for other available options. In this example, the train data set has numbers ranging from 1-10. Whereas in the test data set, there are data points that go out of this range, which the algorithm should be able to detect as anomalies. import numpy as np from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader from numalogic.models.autoencoder import TimeseriesTrainer from numalogic.models.autoencoder.variants import VanillaAE from numalogic.models.threshold import StdDevThreshold from numalogic.transforms import TanhNorm from numalogic.tools.data import StreamingDataset # Create some synthetic data X_train = np . array ([ 1 , 3 , 5 , 2 , 5 , 1 , 4 , 5 , 1 , 4 , 5 , 8 , 9 , 1 , 2 , 4 , 5 , 1 , 3 ]) . reshape ( - 1 , 1 ) X_test = np . array ([ - 20 , 3 , 5 , 60 , 5 , 10 , 4 , 5 , 200 ]) . reshape ( - 1 , 1 ) # Preprocess step clf = StandardScaler () train_data = clf . fit_transform ( X_train ) test_data = clf . transform ( X_test ) print ( train_data ) print ( test_data ) # Set a sequence length. SEQ_LEN = 8 # Define the model. We are using a simple fully connected autoencoder here. model = VanillaAE ( seq_len = SEQ_LEN , n_features = 1 ) # Create a torch dataset train_dataset = StreamingDataset ( train_data , seq_len = SEQ_LEN ) # Define the trainer, and fit the model. trainer = TimeseriesTrainer ( max_epochs = 30 , enable_progress_bar = True ) trainer . fit ( model , train_dataloaders = DataLoader ( train_dataset )) # Get the training reconstruction error from the model. train_reconerr = trainer . predict ( model , dataloaders = DataLoader ( train_dataset , batch_size = 2 ) ) print ( train_reconerr ) # Define threshold estimator, and find a threshold on the training reconstruction error. thresh_clf = StdDevThreshold () thresh_clf . fit ( train_reconerr . numpy ()) # Now it is time for inference on the test data. # First, let's get the reconstruction error on the test set. test_dataset = StreamingDataset ( test_data , seq_len = SEQ_LEN ) test_recon_err = trainer . predict ( model , dataloaders = DataLoader ( test_dataset , batch_size = 2 ) ) print ( test_recon_err ) # The trained threshold estimator can give us the anomaly score anomaly_score = thresh_clf . score_samples ( test_recon_err . numpy ()) # Optionally, we can normalize scores to range between 0-10 to make it more readable postproc_clf = TanhNorm () anomaly_score_norm = postproc_clf . fit_transform ( anomaly_score ) print ( \"Anomaly Scores: \\n \" , str ( anomaly_score_norm )) Below is the sample output, which has logs and anomaly scores printed. Notice the anomaly score for points -20, 40 and 100 in X_test is high. ...snip training logs... Anomaly Scores: [[ 6 .905296 ] [ 0 .1290902 ] [ 0 .17081457 ] [ 9 .688352 ] [ 0 .02224382 ] [ 1 .7376249 ] [ 0 .33091545 ] [ 0 .08399535 ] [ 9 .999992 ]] Replace X_train and X_test with your own data, and see the anomaly scores generated. For more detailed experimentation, refer to quick-start example Numalogic as streaming ML using Numaflow \u00b6 Numalogic can also be paired with our streaming platform Numaflow , to build streaming ML pipelines where Numalogic can be used in UDF . Prerequisite \u00b6 Numaflow Running the Simple Numalogic Pipeline \u00b6 Once Numaflow is installed, create a simple Numalogic pipeline, which takes in time-series data, does the pre-processing, training, inference, and post-processing. For building this pipeline, navigate to numalogic-simple-pipeline under the examples folder and execute the following commands. Apply the pipeline. Note Make sure the pipeline and, numaflow controllers and isbsvc pods are running in the same namespace ( default in this case). kubectl apply -f numa-pl.yaml To verify if the pipeline has been deployed successfully, check the status of each pod. kubectl get pods Output will be something like this: NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3/3 Running 0 68s isbsvc-default-js-1 3/3 Running 0 68s isbsvc-default-js-2 3/3 Running 0 68s mlflow-sqlite-84cf5d6cd-pkmct 1/1 Running 0 46s numalogic-simple-pipeline-preprocess-0-mvuqb 2/2 Running 0 46s numalogic-simple-pipeline-train-0-8xjg1 2/2 Running 0 46s numalogic-simple-pipeline-daemon-66bbd94c4-hf4k2 1/1 Running 0 46s numalogic-simple-pipeline-inference-0-n3asg 2/2 Running 0 46s numalogic-simple-pipeline-threshold-0-ypwl8 2/2 Running 0 46s numalogic-simple-pipeline-postprocess-0-bw67q 2/2 Running 0 46s numalogic-simple-pipeline-out-0-hjb7m 1/1 Running 0 46s numalogic-simple-pipeline-in-0-tmd0v 1/1 Running 0 46s Sending data to the pipeline \u00b6 Once the pipeline has been created, the data can be sent to the pipeline by port-forwarding the input vertex. Port-forward to the http-source vertex. From the above pod output, this would be: kubectl port-forward numalogic-simple-pipeline-in-0-tmd0v 8443 Send the data to the pod via curl curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' Note: only send an array of length 12 in data, as the sequence length used for training is 12. Training \u00b6 Initially, there is no ML model present; to trigger training do a curl command and send any data to the pipeline. The training data is from train_data.csv , which follows a sinusoidal pattern where values fall in the range 200-350. The following logs will be seen in the training pod. > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' > kubectl logs numalogic-simple-pipeline-train-0-xxxxx -c udf 2023 -01-06 18 :36:57,146 - INFO - epoch 0 , loss: 2 .73 2023 -01-06 18 :36:58,069 - INFO - epoch 5 , loss: 0 .00621 2023 -01-06 18 :36:58,918 - INFO - epoch 10 , loss: 0 .00595 2023 -01-06 18 :36:59,735 - INFO - epoch 15 , loss: 0 .00608 2023 -01-06 18 :37:00,547 - INFO - epoch 20 , loss: 0 .00643 2023 -01-06 18 :37:01,339 - INFO - epoch 25 , loss: 0 .00693 2023 -01-06 18 :37:02,146 - INFO - epoch 30 , loss: 0 .0074 2023 -01-06 18 :37:02,956 - INFO - epoch 35 , loss: 0 .00781 2023 -01-06 18 :37:03,754 - INFO - epoch 40 , loss: 0 .0083 2023 -01-06 18 :37:04,551 - INFO - epoch 45 , loss: 0 .00851 ` Trainer.fit ` stopped: ` max_epochs = 50 ` reached. Successfully registered model 'ae::model' . Created version '1' of model 'ae::model' . Successfully registered model 'thresh_clf::model' . Created version '1' of model 'thresh_clf::model' . 2023 -01-06 18 :37:07,957 - INFO - 41d571ca-0e98-4000-bcad-7752e5d5bc81 - Model Saving complete Inference \u00b6 Now, the pipeline is ready for inference with the model trained above, data can be sent to the pipeline for ML inference. After sending the data, look for logs in the output pod, which shows the anomaly score. Since we trained the model with data that follows a sinusoidal pattern where values range from 200-350, any value within this range is considered to be non-anomalous. And any value out of this range is considered to be anomalous. Sending non-anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,359.080987,341.036110,333.584121,376.034150,351.065394,355.379422,333.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:54:44 (out) {\"ts_data\": [[0.14472376660734326], [0.638373062689151], [0.8480656378656608], [0.4205087588581154], [1.285475729481929], [0.8136729095134241], [0.09972157219780131], [0.2856860200353754], [0.6005371351085002], [0.021966491476278518], [0.10405302543443251], [0.6428168263777302]], \"anomaly_score\": 0.49173648784304, \"uuid\": \"0506b380-4565-405c-a3a3-ddc3a19e0bb4\"} Sending anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,800.162220,614.091646,537.250124,776.034150,751.065394,700.379422,733.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:56:40 (out) {\"ts_data\": [[1.173712319431301], [0.39061549013480673], [2.523849648503271], [2.0962927694957254], [13.032012667825805], [5.80166091013039], [3.6868855191928325], [4.814846700913904], [4.185973265627947], [3.9097889275446356], [4.505391607282856], [4.1170053183846305]], \"anomaly_score\": 3.9579276751803145, \"uuid\": \"ed039779-f924-4801-9418-eeef30715ef1\"} In the output, ts_data is the final array that the input array has been transformed to, after all the steps in the pipeline. anomaly_score is the final anomaly score generated for the input data. MLflow UI \u00b6 To see the model in MLflow UI, port forward mlflow-service using the below command and navigate to http://127.0.0.1:5000/ kubectl port-forward svc/mlflow-service 5000 Numaflow UI \u00b6 To see the numaflow pipeline, we can port forward to the UI https://localhost:8000/. kubectl -n numaflow-system port-forward deployment/numaflow-server 8000 :8443 Train on your own data \u00b6 If you want to train an ML model on your own data, replace the train_data.csv file with your own file under resources. For more details, refer to numalogic-simple-pipeline","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"","title":"Quick Start"},{"location":"quick-start/#installation","text":"","title":"Installation"},{"location":"quick-start/#install-pytorch-lightning","text":"Numalogic needs PyTorch and PyTorch Lightning to work. You can install simply by: pip install pytorch-lightning Or you can install your platform specific torch version from their website.","title":"Install pytorch-lightning"},{"location":"quick-start/#install-numalogic","text":"pip install numalogic","title":"Install Numalogic"},{"location":"quick-start/#numalogic-as-a-library","text":"Numalogic can be used as an independent library, and it provides various ML models and tools. Here, we are using the TimeseriesTrainer . Refer to training section for other available options. In this example, the train data set has numbers ranging from 1-10. Whereas in the test data set, there are data points that go out of this range, which the algorithm should be able to detect as anomalies. import numpy as np from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader from numalogic.models.autoencoder import TimeseriesTrainer from numalogic.models.autoencoder.variants import VanillaAE from numalogic.models.threshold import StdDevThreshold from numalogic.transforms import TanhNorm from numalogic.tools.data import StreamingDataset # Create some synthetic data X_train = np . array ([ 1 , 3 , 5 , 2 , 5 , 1 , 4 , 5 , 1 , 4 , 5 , 8 , 9 , 1 , 2 , 4 , 5 , 1 , 3 ]) . reshape ( - 1 , 1 ) X_test = np . array ([ - 20 , 3 , 5 , 60 , 5 , 10 , 4 , 5 , 200 ]) . reshape ( - 1 , 1 ) # Preprocess step clf = StandardScaler () train_data = clf . fit_transform ( X_train ) test_data = clf . transform ( X_test ) print ( train_data ) print ( test_data ) # Set a sequence length. SEQ_LEN = 8 # Define the model. We are using a simple fully connected autoencoder here. model = VanillaAE ( seq_len = SEQ_LEN , n_features = 1 ) # Create a torch dataset train_dataset = StreamingDataset ( train_data , seq_len = SEQ_LEN ) # Define the trainer, and fit the model. trainer = TimeseriesTrainer ( max_epochs = 30 , enable_progress_bar = True ) trainer . fit ( model , train_dataloaders = DataLoader ( train_dataset )) # Get the training reconstruction error from the model. train_reconerr = trainer . predict ( model , dataloaders = DataLoader ( train_dataset , batch_size = 2 ) ) print ( train_reconerr ) # Define threshold estimator, and find a threshold on the training reconstruction error. thresh_clf = StdDevThreshold () thresh_clf . fit ( train_reconerr . numpy ()) # Now it is time for inference on the test data. # First, let's get the reconstruction error on the test set. test_dataset = StreamingDataset ( test_data , seq_len = SEQ_LEN ) test_recon_err = trainer . predict ( model , dataloaders = DataLoader ( test_dataset , batch_size = 2 ) ) print ( test_recon_err ) # The trained threshold estimator can give us the anomaly score anomaly_score = thresh_clf . score_samples ( test_recon_err . numpy ()) # Optionally, we can normalize scores to range between 0-10 to make it more readable postproc_clf = TanhNorm () anomaly_score_norm = postproc_clf . fit_transform ( anomaly_score ) print ( \"Anomaly Scores: \\n \" , str ( anomaly_score_norm )) Below is the sample output, which has logs and anomaly scores printed. Notice the anomaly score for points -20, 40 and 100 in X_test is high. ...snip training logs... Anomaly Scores: [[ 6 .905296 ] [ 0 .1290902 ] [ 0 .17081457 ] [ 9 .688352 ] [ 0 .02224382 ] [ 1 .7376249 ] [ 0 .33091545 ] [ 0 .08399535 ] [ 9 .999992 ]] Replace X_train and X_test with your own data, and see the anomaly scores generated. For more detailed experimentation, refer to quick-start example","title":"Numalogic as a Library"},{"location":"quick-start/#numalogic-as-streaming-ml-using-numaflow","text":"Numalogic can also be paired with our streaming platform Numaflow , to build streaming ML pipelines where Numalogic can be used in UDF .","title":"Numalogic as streaming ML using Numaflow"},{"location":"quick-start/#prerequisite","text":"Numaflow","title":"Prerequisite"},{"location":"quick-start/#running-the-simple-numalogic-pipeline","text":"Once Numaflow is installed, create a simple Numalogic pipeline, which takes in time-series data, does the pre-processing, training, inference, and post-processing. For building this pipeline, navigate to numalogic-simple-pipeline under the examples folder and execute the following commands. Apply the pipeline. Note Make sure the pipeline and, numaflow controllers and isbsvc pods are running in the same namespace ( default in this case). kubectl apply -f numa-pl.yaml To verify if the pipeline has been deployed successfully, check the status of each pod. kubectl get pods Output will be something like this: NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3/3 Running 0 68s isbsvc-default-js-1 3/3 Running 0 68s isbsvc-default-js-2 3/3 Running 0 68s mlflow-sqlite-84cf5d6cd-pkmct 1/1 Running 0 46s numalogic-simple-pipeline-preprocess-0-mvuqb 2/2 Running 0 46s numalogic-simple-pipeline-train-0-8xjg1 2/2 Running 0 46s numalogic-simple-pipeline-daemon-66bbd94c4-hf4k2 1/1 Running 0 46s numalogic-simple-pipeline-inference-0-n3asg 2/2 Running 0 46s numalogic-simple-pipeline-threshold-0-ypwl8 2/2 Running 0 46s numalogic-simple-pipeline-postprocess-0-bw67q 2/2 Running 0 46s numalogic-simple-pipeline-out-0-hjb7m 1/1 Running 0 46s numalogic-simple-pipeline-in-0-tmd0v 1/1 Running 0 46s","title":"Running the Simple Numalogic Pipeline"},{"location":"quick-start/#sending-data-to-the-pipeline","text":"Once the pipeline has been created, the data can be sent to the pipeline by port-forwarding the input vertex. Port-forward to the http-source vertex. From the above pod output, this would be: kubectl port-forward numalogic-simple-pipeline-in-0-tmd0v 8443 Send the data to the pod via curl curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' Note: only send an array of length 12 in data, as the sequence length used for training is 12.","title":"Sending data to the pipeline"},{"location":"quick-start/#training","text":"Initially, there is no ML model present; to trigger training do a curl command and send any data to the pipeline. The training data is from train_data.csv , which follows a sinusoidal pattern where values fall in the range 200-350. The following logs will be seen in the training pod. > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' > kubectl logs numalogic-simple-pipeline-train-0-xxxxx -c udf 2023 -01-06 18 :36:57,146 - INFO - epoch 0 , loss: 2 .73 2023 -01-06 18 :36:58,069 - INFO - epoch 5 , loss: 0 .00621 2023 -01-06 18 :36:58,918 - INFO - epoch 10 , loss: 0 .00595 2023 -01-06 18 :36:59,735 - INFO - epoch 15 , loss: 0 .00608 2023 -01-06 18 :37:00,547 - INFO - epoch 20 , loss: 0 .00643 2023 -01-06 18 :37:01,339 - INFO - epoch 25 , loss: 0 .00693 2023 -01-06 18 :37:02,146 - INFO - epoch 30 , loss: 0 .0074 2023 -01-06 18 :37:02,956 - INFO - epoch 35 , loss: 0 .00781 2023 -01-06 18 :37:03,754 - INFO - epoch 40 , loss: 0 .0083 2023 -01-06 18 :37:04,551 - INFO - epoch 45 , loss: 0 .00851 ` Trainer.fit ` stopped: ` max_epochs = 50 ` reached. Successfully registered model 'ae::model' . Created version '1' of model 'ae::model' . Successfully registered model 'thresh_clf::model' . Created version '1' of model 'thresh_clf::model' . 2023 -01-06 18 :37:07,957 - INFO - 41d571ca-0e98-4000-bcad-7752e5d5bc81 - Model Saving complete","title":"Training"},{"location":"quick-start/#inference","text":"Now, the pipeline is ready for inference with the model trained above, data can be sent to the pipeline for ML inference. After sending the data, look for logs in the output pod, which shows the anomaly score. Since we trained the model with data that follows a sinusoidal pattern where values range from 200-350, any value within this range is considered to be non-anomalous. And any value out of this range is considered to be anomalous. Sending non-anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,359.080987,341.036110,333.584121,376.034150,351.065394,355.379422,333.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:54:44 (out) {\"ts_data\": [[0.14472376660734326], [0.638373062689151], [0.8480656378656608], [0.4205087588581154], [1.285475729481929], [0.8136729095134241], [0.09972157219780131], [0.2856860200353754], [0.6005371351085002], [0.021966491476278518], [0.10405302543443251], [0.6428168263777302]], \"anomaly_score\": 0.49173648784304, \"uuid\": \"0506b380-4565-405c-a3a3-ddc3a19e0bb4\"} Sending anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,800.162220,614.091646,537.250124,776.034150,751.065394,700.379422,733.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:56:40 (out) {\"ts_data\": [[1.173712319431301], [0.39061549013480673], [2.523849648503271], [2.0962927694957254], [13.032012667825805], [5.80166091013039], [3.6868855191928325], [4.814846700913904], [4.185973265627947], [3.9097889275446356], [4.505391607282856], [4.1170053183846305]], \"anomaly_score\": 3.9579276751803145, \"uuid\": \"ed039779-f924-4801-9418-eeef30715ef1\"} In the output, ts_data is the final array that the input array has been transformed to, after all the steps in the pipeline. anomaly_score is the final anomaly score generated for the input data.","title":"Inference"},{"location":"quick-start/#mlflow-ui","text":"To see the model in MLflow UI, port forward mlflow-service using the below command and navigate to http://127.0.0.1:5000/ kubectl port-forward svc/mlflow-service 5000","title":"MLflow UI"},{"location":"quick-start/#numaflow-ui","text":"To see the numaflow pipeline, we can port forward to the UI https://localhost:8000/. kubectl -n numaflow-system port-forward deployment/numaflow-server 8000 :8443","title":"Numaflow UI"},{"location":"quick-start/#train-on-your-own-data","text":"If you want to train an ML model on your own data, replace the train_data.csv file with your own file under resources. For more details, refer to numalogic-simple-pipeline","title":"Train on your own data"},{"location":"threshold/","text":"Threshold Estimators \u00b6 Threshold Estimators are used for identifying the threshold limit above which we regard the datapoint as anomaly. It is a simple Estimator that extends BaseEstimator. Currently, the library supports StdDevThreshold . This takes in paramaters min_thresh and std_factor . This model defines threshold as mean + 3 * std_factor . import numpy as np from numalogic.models.threshold import StdDevThreshold # Generate positive random data x_train = np . abs ( np . random . randn ( 1000 , 3 )) x_test = np . abs ( np . random . randn ( 30 , 3 )) # Here we want a threshold such that anything # outside 5 deviations from the mean will be anomalous. thresh_clf = StdDevThreshold ( std_factor = 5 ) thresh_clf . fit ( x_train ) # Let's get the predictions y_pred = thresh_clf . predict ( x_test ) # Anomaly scores can be given by, score_samples method y_score = thresh_clf . score_samples ( x_test )","title":"Threshold Estimators"},{"location":"threshold/#threshold-estimators","text":"Threshold Estimators are used for identifying the threshold limit above which we regard the datapoint as anomaly. It is a simple Estimator that extends BaseEstimator. Currently, the library supports StdDevThreshold . This takes in paramaters min_thresh and std_factor . This model defines threshold as mean + 3 * std_factor . import numpy as np from numalogic.models.threshold import StdDevThreshold # Generate positive random data x_train = np . abs ( np . random . randn ( 1000 , 3 )) x_test = np . abs ( np . random . randn ( 30 , 3 )) # Here we want a threshold such that anything # outside 5 deviations from the mean will be anomalous. thresh_clf = StdDevThreshold ( std_factor = 5 ) thresh_clf . fit ( x_train ) # Let's get the predictions y_pred = thresh_clf . predict ( x_test ) # Anomaly scores can be given by, score_samples method y_score = thresh_clf . score_samples ( x_test )","title":"Threshold Estimators"}]}