stream_confs:
  druid-config:
    config_id: "druid-config"
    source: "druid"
    composite_keys: [ 'service-mesh', '1', '2' ]
    window_size: 20
    ml_pipelines:
      pipeline1:
        pipeline_id: "pipeline1"
        metrics: [ "degraded" , "failed" ]
        numalogic_conf:
          model:
            name: "VanillaAE"
            conf:
              seq_len: 20
              n_features: 1
          preprocess:
            - name: "FlattenVector"
              stateful: false
              conf:
                n_features: 2
            - name: "LogTransformer"
              stateful: false
              conf:
                add_factor: 5
          threshold:
            name: "StdDevThreshold"
            conf:
              min_threshold: 0.1
          score:
            feature_agg:
              method: MAX
            adjust:
              upper_limits:
                "col1": 23
          trainer:
            train_hours: 3
            min_train_size: 100
            transform:
              - name: DataClipper
                conf:
                  lower: [0.0, -inf]
                  upper: [0.0, inf]
            pltrainer_conf:
              accelerator: cpu
              max_epochs: 5
      pipeline2:
        pipeline_id: "pipeline2"
        metrics: [ "col1" , "col2" ]
        numalogic_conf:
          model:
            name: "VanillaAE"
            conf:
              seq_len: 10
              n_features: 2
          preprocess:
            - name: "LogTransformer"
              stateful: false
              conf:
                add_factor: 5
          threshold:
            name: "StdDevThreshold"
            conf:
              min_threshold: 0.1
          score:
            feature_agg:
              method: MEAN
            adjust:
              upper_limits:
                "col1": 20
                "col2": 18


redis_conf:
  url: "isbsvc-redis-isbs-redis-svc.oss-analytics-numalogicosamfci-usw2-e2e.svc"
  port: 26379
  expiry: 360
  master_name: "mymaster"

druid_conf:
  url: "druid-endpoint"
  endpoint: "endpoint"
  id_fetcher:
    druid-config-pipeline1:
      dimensions: [ "col1" ]
      datasource: "table-name"
      group_by: [ "timestamp", "col1" ]
      pivot:
        columns: [ "col2" ]
